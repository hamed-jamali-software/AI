{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzueOHgg9i1Krmhy3ZxGvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamed-jamali-software/AI/blob/main/learn_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cV01PBLlqSzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3bd8331-5481-41ce-a6c1-8eb0622053cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-09c5ujn7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-09c5ujn7\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=b842a214f4c371a80ca026f3acb46bb45197dc72d7c21eb48bf710f3b8d3fabd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nd1gcj_/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper"
      ],
      "metadata": {
        "id": "AiqB6OIbxAGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('large')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2VOKpykxpoQ",
        "outputId": "5c0ad54a-c8ef-466d-a56f-b724ab18d1a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [01:14<00:00, 41.2MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe('/content/Projekt.m4a')\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llovpi1YyNOv",
        "outputId": "6671a7fd-e988-4aa4-8c1b-ec6ed684b14a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Also, erster Schritt wäre, dass man Lieder nimmt, die man recherchiert, frei zugänglich am besten, keine lizenzrechtlichen Probleme, sondern freie Lieder, wo auch Gesang und Audiodateien da ist. Ja, Vocals. Dann gibt es Modelle, ich weiß leider nicht den Namen momentan, aber es gibt Modelle, mit denen Sie Vocals von Instrumental trennen können. Dann haben Sie zwei Audiodateien. Dann haben Sie eine Audiodatei, da ist nur der Gesang drin und eine Audiodatei, in der nur die Musik ist. Ja. Dann nehmen Sie diese Audiodatei, wo der Gesang drin ist und geben die an Visper. Ja. Visper. Visper. Visper, W-H-I-S-P-E-R. Ja. Visper ist ein Transkription-Modell. Ja. Dann nehmen Sie mal Visper von OpenAI. Visper. Vis. Ne, nicht. Vis. Kann ich? Ja. Kann ich? Ja. Und Visper ist ein ASR, ein Text-to-Speech. Ja. Ein Speech-to-Text. Ja. Ja. Ja. Also, Speech-to-Text. Dann kriegen Sie Text. Ja. Und dann haben Sie ja die Original-Audiodatei. Ja. Dann haben Sie den Text zu der passenden Audiodatei. Ah, okay. Und dann könnte man dadurch das nutzen, um so ein Modell zu trainieren. Ja. Ja. Also, das wäre erster Schritt, aber natürlich erstmal die ganzen Möglichkeiten aufzeigen. Ja. Und dann der zweite Schritt wäre, könnte man überlegen, dass man so eine Landenbank aufbaut. Ja. Oder Daten aufbaut und dann tatsächlich das Modell trainiert. Ja. Ja. Und dann, die Daten, die lassen sich tatsächlich, die sind nicht so riesig, die lassen sich tatsächlich auch wahrscheinlich mit der Hardware, die wir haben, trainieren. Hardware? Hardware. Hardware. Server. Server. Ja, weil Sie brauchen ja für Sprachmodelle brauchen Sie ja wahnsinnig viele Server, um zu trainieren. Ja. Da brauchen Sie ja tausende von Grafikkarten. Aber für die Audio-Modelle braucht man das gar nicht so stark. Die lassen sich auch mit weniger Grafikkarten trainieren. Ja. Und das könnten wir wahrscheinlich sogar hier machen. Ja. Ja. Aber das ist ein guter Schritt, wenn man nicht sogar so ein Modell trainiert. Ja. Aber das ist der erste Schritt, erstmal alles zusammenpacken. Ja. Ja. Architekturen, rausfinden, wie man Texte mit, also Vocals mit in den Diffusion-Prozess reinpricht. Ja. Aber ich glaube, dass diese Stable Diffusion die richtigen Modelle sind. Ja. Ja. Ja. Um das so weit zu kriegen, weil, und dann mal gucken, ob es nicht hier Variationen gibt dazu. Ja. Ja? Ja. Ja. Ja. Ja. Ja. Fures Muse-S Nations, also Stable Audio-Prozess. Ja. Also auch, ich glaube, was ich benutzte, ist, dass probably, da geht es durch Video, jetzt irgendwo über die Initzer bands und gitu, dann hat es noch verschiedene Signals und so weiter. Do man nicht so schnell die von eigentlich podcasts machen? Das stimmt. Genau. describing. Genau, dann nochmal onlineandenback, wenn man es tauschen wollte. Klar. Gut. Und? Sind das alles Link alimentos? Nichts? Also geben Sie mal bei Papers with Code Stable Audio ein. So und jetzt Stable Audio. Starplay. L. I. Ja und dann der. Und dann Audio. Ja. Ich weiß nicht wieviel. Ja, klingt super als erstes an. Das ist richtig. Das kann ich mal durch. Da ist es. Ja. Das ist es. Ja, sehr gut. Ja. Ja und die Schwierigkeit bei, also die Diffusion Modelle sind ja auf Bild ausgelegt. Ja. Und jetzt hat man ja das Problem, dass Bilder ja keine Zeitachse haben. Die sind ja nur auf Bild ausgelegt. Ja. Nur eine Dimension, also zwei Dimensionen. Ja. Keine, Zeit ist ja nochmal eine Dimension. Dann hätten wir ja drei Dimensionen. Das ist, drei Dimensionen? Nein, aber Sie haben ja das Problem, also die Probleme, dass man Diffusion Modelle hat, hat damit zu tun, dass wenn Sie, ein Diffusion Modell funktioniert so, man nimmt ein Bild. Ja. Und legt ein Rauschen drüber. Ja. Und dann soll das Modell landen, das Rauschen zu entfernen. Ah. Und dann macht man ein Diffusion Modell. Ja, also Sie haben, der Trainingsprozess sieht so aus, Sie haben einerseits ein, ein, ein voll verrauschtes Bild. Ja. Da sind nur schwarz-weiß Punkte drauf, sonst nichts. Und ein Bild, das so aussieht. Und dann sagt man dem Modell, das ist der Input. Ja. Und das ist das Ziel. Ja. Erzeuge das Bild. Und dann versucht er die Punkte zu entfernen. Ja. Um das Bild wieder herzustellen. Ja. Und das lernt er dann. Das sind Diffusion Modelle. Aber wenn wir ein Bild haben, dann ist das ja eine Fläche. Also Sie müssen sich ja vorstellen, wie jetzt hier. Das wäre ja ein Bild. Ja. Aber Audio ist ja kein Bild. Ja, ich weiß. Sondern Audio ist ja kontinuierlich. Ja, ja. Das heißt, man braucht einen, einen, ja, time-abhängigen Raum. Ja. Also so haben wir beim Bild, hätten wir ja zwei Dimensionen. Dimension eins, Dimension zwei. Ja. Aber jetzt kommt ja die Zeit dazu. Ja, ja. Und das wäre die dritte Dimension. Ja, das ist x-weise. Ja, x-weise, genau. Das ist nicht so. Und das ist das, was dieses Paper erklärt. Ja. Indem man die Zeit da reinkriegt. Ja. Weil das ist genau dieses Zeitfenster. Ja. Und jetzt könnte man das reinkriegen, indem man das halt so wie die machen. Es gibt aber auch die Möglichkeit, machen Sie nochmal eins auf. Darf ich Ihnen mal. Ich habe evaluiert, meine Idee kommt her. Ja, können Sie da mal drauf drücken. So, und dann auf Bilder, Bilder, Bilder, Images. Ja, das habe ich gerade auch. So, das sind sogenannte Mail-Spektrogramme. Ja, ja, ja. Die kombinieren die Audio-Welle mit der Zeit. Das ist Dynamik und Zeit. Das heißt, man hat ein Bild. Jetzt könnte man auch so ein Bild benutzen zum Trainieren. Also, audio in Bild transferieren und dann das Bild zum Trainieren benutzen. Auch das wäre eine Möglichkeit. Da gibt es auch Papers. Und da müssten Sie jetzt mal versuchen, so ein bisschen Ordnung reinzukriegen. Ja, ja. Okay? Ja, ist okay. Gut. Viel Spaß. Vielen Dank. Danke. Gerne.\n"
          ]
        }
      ]
    }
  ]
}